#!/usr/bin/python3
import argparse
import os
import sys
import time
import logging
from Bio import SeqIO

from mbcclr_utils import runners_utils
from mbcclr_utils import sample_data
from mbcclr_utils import binner_core

def main():
    parser = argparse.ArgumentParser(description="""MetaBCC-LR Help. A tool developed for binning of metagenomics long reads (PacBio/ONT). \
            Tool utilizes composition and coverage profiles of reads based on k-mer frequencies to perform dimension reduction. \
            dimension reduced reads are then clustered using DB-SCAN. Minimum RAM requirement is 9GB.""")

    parser.add_argument('--reads-path', '-r',
                        help="Reads path for binning",
                        type=str,
                        required=True)
    parser.add_argument('--embedding', '-e',
                        help="Embedding tool to be used for clustering",
                        choices=['tsne', 'umap', 'song'],
                        type=str,
                        required=False,
                        default="tsne")
    parser.add_argument('--k-size', '-k',
                        help="Choice of k-mer for oligonucleotide frequency vector.",
                        type=int,
                        choices=[3,4,5,6,7],
                        required=False,
                        default=3)  
    parser.add_argument('--sample-count', '-c',
                        help="Number of reads to sample in order to determine the number of bins. Set to 1%% of reads by default. Changing this parameter will\
                            affect whether low coverage species are separated or not.",
                        type=int,
                        required=False,
                        default=0)         
    parser.add_argument('--sensitivity', '-s',
                        help="Value between 1 and 10, Higher helps recovering low abundant species (No. of species > 100)",
                        type=int,
                        required=False,
                        default=5)    
    parser.add_argument('--bin-size', '-bs',
                        help="Size of each bin in coverage histogram.",
                        type=int,
                        required=False,
                        default=10) 
    parser.add_argument('--bin-count', '-bc',
                        help="Number of bins in the coverage histogram.",
                        type=int,
                        required=False,
                        default=32)                
    # parser.add_argument('--max-memory', '-m',
    #                     help="Default 5000. DSK k-mer counter accepts a max memory parameter. However, the complete pipeline requires 5GB+ RAM. \
    #                         This is only to make DSK step faster, should you have more RAM.",
    #                     type=int,
    #                     required=False,
    #                     default=5000)                        
    parser.add_argument('--threads', '-t',
                        help="Thread count for computation",
                        type=int,
                        default=8,
                        required=False)
    parser.add_argument('--ground-truth', '-g',
                        help="Ground truth of reads for dry runs and sensitivity tuning",
                        type=str,
                        required=False,
                        default=None)
    parser.add_argument('--resume',
                        action='store_true',
                        help='Continue from the last step or the binning step (which ever comes first). Can save time needed to run DSK and obtain k-mers. Ideal for sensitivity tuning'
                        )                       
    parser.add_argument('--output', '-o', help="Output directory", type=str, required=True)
    parser.add_argument('--version', '-v',
                        action='version',
                        help="Show version.",
                        version='%(prog)s 2.0')             

    args = parser.parse_args()

    reads_path = args.reads_path
    output = args.output
    threads = args.threads
    k_size = args.k_size
    ground_truth = args.ground_truth
    sample_count = args.sample_count
    sensitivity = args.sensitivity
    resume = args.resume
    bin_size = args.bin_size
    bin_count = args.bin_count
    embedding = args.embedding
    # max_memory = max(args.max_memory, 5000)
    checkpoints_path = f"{output}/checkpoints"

    logger = logging.getLogger('MetaBCC-LR')
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    consoleHeader = logging.StreamHandler()
    consoleHeader.setFormatter(formatter)
    consoleHeader.setLevel(logging.INFO)
    logger.addHandler(consoleHeader)

    start_time = time.time()

    # Validation of inputs
    if threads <= 0:
        print("Minimum number of threads is 1. Using thread count 1 and continue")
        threads = 1

    if not (sensitivity >= 1 and sensitivity <= 10):
        print("Sensitivity must be within 1-10 inclusive")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if not os.path.isfile(reads_path):
        print("Failed to open reads file")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if ground_truth and not os.path.isfile(ground_truth):
        print("Failed to open read ids. Continue without ids")
        ground_truth = None

    if not os.path.exists(output):
        os.makedirs(output)
    if not os.path.exists(f"{output}/images"):
        os.makedirs(f"{output}/images")
    if not os.path.exists(f"{output}/misc"):
        os.makedirs(f"{output}/misc")

    fileHandler = logging.FileHandler(f"{output}/metabcc-lr.log")
    fileHandler.setLevel(logging.DEBUG)
    fileHandler.setFormatter(formatter)
    logger.addHandler(fileHandler)

    if not reads_path.split(".")[-1].lower() in ['fq', 'fasta', 'fa', 'fastq']:
        logger.error("Unable to detect file type of reads. Please use either FASTA of FASTQ. Good Bye!")
        sys.exit(1)
    
    logger.info("Command " + " ".join(sys.argv))

    if not resume:
        data = {}
        data['reads_path'] = reads_path
        data['output'] = output
        data['threads'] = threads
        data['truth'] = ground_truth
        
        # coverage related
        data['bin_size'] = bin_size
        data['bin_count'] = bin_count
        
        data['sample_count'] = sample_count
        data['sensitivity'] = sensitivity
        data['completed'] = set()

        runners_utils.checkpoint(data, checkpoints_path)

    # running program
    start_time = time.time()

    if resume:
        logger.info("Resuming the program from previous checkpoints")
        data = runners_utils.load_checkpoints(checkpoints_path)
        logger.debug(str(data))

    # moslty useless step as most reads are longer now
    # if resume and "filtering" not in data['completed'] or not resume:
    #     logger.info("Filtering reads")
    #     runners_utils.run_filter(reads_path, output, ground_truth)
    #     data['completed'].add('filtering')
    #     runners_utils.checkpoint(data, checkpoints_path)
    #     logger.info("Filtering reads complete")

    # now we have a faster implementation
    # if resume and "dsk" not in data['completed'] or not resume:
    #     logger.info("Running DSK k-mer counting")
    #     runners_utils.run_dsk(output, max_memory, threads)
    #     logger.info("Running DSK k-mer counting complete")
    #     data['completed'].add('dsk')
    #     runners_utils.checkpoint(data, checkpoints_path)
    #     logger.info("Running DSK k-mer complete")

    if resume and "kmers" not in data['completed'] or not resume:
        logger.info("Counting K-mers")
        runners_utils.run_kmers(reads_path, output, k_size, threads)
        data['completed'].add('kmers')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting K-mers complete")

    if resume and ("15mers" not in data['completed'] or data['bin_size'] != bin_size or data['bin_count'] != bin_count) or not resume:
        logger.info("Counting 15-mer profiles")
        data['completed'].add('15mers')
        data['bin_size'] = bin_size
        data['bin_count'] = bin_count
        runners_utils.run_15mers(reads_path, output, bin_size, bin_count, threads)
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting 15-mer profiles complete")

    if resume and "sample" not in data['completed'] or not resume or resume and data['sample_count'] != sample_count:
        logger.info("Sampling Reads")
        sample_data.sample(output, sample_count, ground_truth)
        data['completed'].add('sample')
        data['sample_count'] = sample_count
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Sampling reads complete")

    logger.info("Binning sampled reads")
    binner_core.run_binner(output, ground_truth, threads, sensitivity, embedding)
    logger.info("Binning sampled reads complete")

    logger.info("Predict read bins")
    runners_utils.run_assign(output, threads)
    logger.info("Predict read bins complete")
    
    end_time = time.time()
    time_taken = end_time - start_time
    logger.info(f"Program Finished!. Please find the output in {output}/final.txt")
    logger.info(f"Total time consumed = {time_taken:10.2f} seconds")

    logger.removeHandler(fileHandler)
    logger.removeHandler(consoleHeader)

if __name__ == '__main__':
    main()
