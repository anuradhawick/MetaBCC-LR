#!/usr/bin/python3
import argparse
import os
import sys
import time
import logging
from Bio import SeqIO

from mbcclr_utils import runners_utils
from mbcclr_utils import sample_data
from mbcclr_utils import binner_core

def main():
    parser = argparse.ArgumentParser(description="""MetaBCC-LR Help. A tool developed for binning of metagenomics long reads (PacBio/ONT). \
            Tool utilizes composition and coverage profiles of reads based on k-mer frequencies to perform dimension reduction. \
            dimension reduced reads are then clustered using DB-SCAN. Minimum RAM requirement is 9GB.""")

    parser.add_argument('--reads-path', '-r',
                        help="Reads path for binning",
                        type=str,
                        required=True)
    parser.add_argument('--threads', '-t',
                        help="Thread count for computation",
                        type=int,
                        default=8,
                        required=False)
    parser.add_argument('--ground-truth', '-g',
                        help="Ground truth of reads for dry runs and sensitivity tuning",
                        type=str,
                        required=False,
                        default=None)
    parser.add_argument('--sample-count', '-c',
                        help="Number of reads to sample in order to determine the number of bins. Set to 1%% of reads by default. Changing this parameter will\
                            affect whether low coverage species are separated or not.",
                        type=int,
                        required=False,
                        default=0)         
    parser.add_argument('--sensitivity', '-s',
                        help="Value between 1 and 10, Higher helps recovering low abundant species (No. of species > 100)",
                        type=int,
                        required=False,
                        default=10)    
    parser.add_argument('--bin-size', '-b',
                        help="Value of bx32 will be the total coverage of k-mers in the coverage histograms. \
                            Usually k-mers are shifted towards y-axis due to errors. By defaul b=10; coverages upto 320X",
                        type=int,
                        required=False,
                        default=10)                
    parser.add_argument('--max-memory', '-m',
                        help="Default 5000. DSK k-mer counter accepts a max memory parameter. However, the complete pipeline requires 5GB+ RAM. \
                            This is only to make DSK step faster, should you have more RAM.",
                        type=int,
                        required=False,
                        default=5000)                        
    parser.add_argument('--resume',
                        action='store_true',
                        help='Continue from the last step or the binning step (which ever comes first). Can save time needed to run DSK and obtain k-mers. Ideal for sensitivity tuning'
                        )                       
    parser.add_argument('--output', '-o', metavar='<DEST>', help="Output directory", type=str, required=True)
    parser.add_argument('--version', '-v',
                        action='version',
                        help="Show version.",
                        version='%(prog)s 1.0.0')             

    args = parser.parse_args()

    reads_path = args.reads_path
    output = args.output
    threads = args.threads
    ground_truth = args.ground_truth
    sample_count = args.sample_count
    sensitivity = args.sensitivity
    resume = args.resume
    bin_size = args.bin_size
    max_memory = max(args.max_memory, 5000)
    checkpoints_path = f"{output}/checkpoints"

    logger = logging.getLogger('MetaBCC-LR')
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    consoleHeader = logging.StreamHandler()
    consoleHeader.setFormatter(formatter)
    consoleHeader.setLevel(logging.INFO)
    logger.addHandler(consoleHeader)

    start_time = time.time()

    # Validation of inputs
    if threads <= 0:
        print("Minimum number of threads is 1. Using thread count 1 and continue")
        threads = 1

    if not (sensitivity >= 1 and sensitivity <= 10):
        print("Sensitivity must be within 1-10 inclusive")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if not os.path.isfile(reads_path):
        print("Failed to open reads file")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if ground_truth and not os.path.isfile(ground_truth):
        print("Failed to open read ids. Continue without ids")
        ground_truth = None

    if not os.path.exists(output):
        os.makedirs(output)
        os.makedirs(f"{output}/images")
        os.makedirs(f"{output}/misc")

    fileHandler = logging.FileHandler(f"{output}/metabcc-lr.log")
    fileHandler.setLevel(logging.DEBUG)
    fileHandler.setFormatter(formatter)
    logger.addHandler(fileHandler)

    if not reads_path.split(".")[-1].lower() in ['fq', 'fasta', 'fa', 'fastq']:
        logger.error("Unable to detect file type of reads. Please use either FASTA of FASTQ. Good Bye!")
        sys.exit(1)

    if not resume:
        data = {}
        data['r'] = reads_path
        data['o'] = output
        data['t'] = threads
        data['i'] = ground_truth
        data['c'] = sample_count
        data['s'] = sensitivity
        data['completed'] = set()

        runners_utils.checkpoint(data, checkpoints_path)

    # running program
    start_time = time.time()

    if resume:
        logger.info("Resuming the program from previous checkpoints")
        data = runners_utils.load_checkpoints(checkpoints_path)
        logger.debug(str(data))

    if resume and "filtering" not in data['completed'] or not resume:
        logger.info("Filtering reads")
        runners_utils.run_filter(reads_path, output, ground_truth)
        data['completed'].add('filtering')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Filtering reads complete")

    if resume and "dsk" not in data['completed'] or not resume:
        logger.info("Running DSK k-mer counting")
        runners_utils.run_dsk(output, max_memory, threads)
        logger.info("Running DSK k-mer counting complete")
        data['completed'].add('dsk')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Running DSK k-mer complete")

    if resume and "trimers" not in data['completed'] or not resume:
        logger.info("Counting Trimers")
        runners_utils.run_3mers(output, threads)
        data['completed'].add('trimers')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting Trimers complete")

    if resume and "15mers" not in data['completed'] or not resume:
        logger.info("Counting 15-mer profiles")
        data['completed'].add('15mers')
        runners_utils.run_15mers(output, bin_size, threads)
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting 15-mer profiles complete")

    if resume and "sample" not in data['completed'] or not resume or resume and data['c'] != sample_count:
        logger.info("Sampling Reads")
        sample_data.sample(output, sample_count, ground_truth)
        data['completed'].add('sample')
        data['c'] = sample_count
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Sampling reads complete")

    logger.info("Binning sampled reads")
    binner_core.run_binner(output, ground_truth, threads, sensitivity)
    logger.info("Binning sampled reads complete")

    logger.info("Predict reads' bins")
    runners_utils.run_assign(output, threads)
    logger.info("Predict reads' bins")

    logger.info("Generating final output")
    with open(f"{output}/final.txt", "w+") as final_result:
        for record, bin_id in zip(SeqIO.parse(f"{output}/misc/filtered_reads.fasta", "fasta"), open(f"{output}/misc/final.txt")):
            final_result.write(f"{record.id}\t{bin_id}")
    
    end_time = time.time()
    time_taken = end_time - start_time
    logger.info(f"Program Finished!. Please find the output in {output}/final.txt")
    logger.info(f"Total time consumed = {time_taken:10.2f} seconds")

    logger.removeHandler(fileHandler)
    logger.removeHandler(consoleHeader)

if __name__ == '__main__':
    main()